---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
#Library Imports
library(caret)
library(dplyr)
library(class)
library(psych)
library(corrplot)

#Load dataset, with the dataset being in the current working directory
ds <- read.csv("heart_disease.csv")
ds1 <- ds #For variable summary in initial dataset

#Filter out duplicates and NA values, if present
ds <- ds[complete.cases(ds),]
ds<-ds %>% select(age,cp,trtbps,chol,thalachh,output) %>% distinct()

#make the predictors as numeric, and the output as levels (factors)
ds[,6] <- factor(ds[,6])
ds[,1:5] <- sapply(ds[,1:5],as.numeric)

summary(ds1)
str(ds1)

```

```{r}
##Data visualization (Figures 1 and 8)

#Scatterplot in the extracted dataset
pairs.panels(ds[,c(1,3,4,5)],
  method = "pearson", # correlation method
  hist.col = "steelblue",
  pch = 21, bg = c("pink", "light green", "light blue")[(iris$Species)],
  density = TRUE, # show density plots
  ellipses = FALSE # show correlation ellipses
)

#Box plots (Figures 6 and 7)
ggplot(heart1,aes(x=output,y=thalachh))+geom_boxplot(aes(col=output))
ggplot(heart1,aes(x=output,y=age))+geom_boxplot(aes(col=output))
ggplot(heart1,aes(x=output,y=chol))+geom_boxplot(aes(col=output))
ggplot(heart1,aes(x=output,y=trtbps))+geom_boxplot(aes(col=output))
```

```{r}
#training and testing
set.seed(101)
indices <- sample(1:nrow(ds),nrow(ds)*0.8)
traindata <- ds[indices,]
testdata <- ds[-indices,]

#Dimensions of train data and test data
dim(traindata)
dim(testdata)
```

```{r}
#cluster grouping the datasets

#Scaling the dataset 
ds2 <- scale(ds) #specify a new dataset for scaled values

#Function definition for clustering
wcss <- function(k){
  kmeans(ds2,k,nstart=25)$tot.withinss
}

#Graph showing the total within clusters sum of squares (Figure 3)
kvalues <- 1:15
set.seed(101)
wcss_k <- sapply(kvalues,wcss)
plot(kvalues,wcss_k,type='b',pch=19,frame=FALSE,xlab="Number of Clusters K",ylab = "Total within-clusters sum of squares")

#The best number of clusters was 4, from the elbow method
set.seed(101)
k4final <- kmeans(ds2,4,nstart=40)
#Outputting the 4 clusters obtained from k means
k4final
```


```{r}
#KNN
#Normalize numeric variables for kNN classification
nor <- function(x) {(x - min(x))/(max(x)-min(x))}
ds1 <- ds #non-normalized dataset is needed for Logistic Regression and SVM, so opy the one used for kNN into a separate variable
ds1[,1:5] <- sapply(ds1[,1:5], nor)

#get the same training and test data from the previously generated indices, but from ds1
traindata2 <- ds1[indices,]
testdata2 <- ds1[-indices,]

#kNN classification
#Try different k
ac <- rep(0,30) #list to store the accuracy for each k
tb<-rep(0, 30) #list to store the false negatives from the confusion matrix for each k
k <- rep(0,30)
for(i in 1:30){
  set.seed(101)
  knn.i <- knn(traindata2[,1:5], testdata2[,1:5], cl=traindata2$output, k = i)
  ac[i] <- mean(knn.i == testdata2$output)
  #get the confusion matrix and the number of false negatives
  currentTable<-table(knn.i,testdata2$output)
  tb[i]<-currentTable[1,2]
  k[i]<- i
  cat("k = ", i, "accuracy = ", ac[i], "False negative", tb[i], "\n")
}

#Accuracy plot (Figures 2a and 4)
nor.tb <- nor(tb) 
nor.ac <- nor(ac) 
plot(k, nor.tb, type="b", pch=19, col="red", xlab="k", ylab="Normalized values")
# Add another plot
lines(k, nor.ac, pch=19, col="blue", type="b", lty=2)
# Add line for optimal k
abline(v = c(6), col = "blue", lty = c(1,2), lwd = c(1,3))
# Label axis
mtext("k = 6", side = 1, line = 0, at = 6)
# Add a legend
legend(x = "top", legend=c("False negative", "Accuracy"),
       col=c("red", "blue"), lty=1:2, cex=0.8)




#kNN model for k = 6 (best model)
set.seed(101)
knn1 <- knn(traindata2[,1:5], testdata2[,1:5], cl = traindata2$output, k=6)
mean(knn1 == testdata2$output)

#table
table(knn1, testdata2$output)
```

```{r}
#logistic regression
model1 <- glm(output~age+cp+trtbps+chol+thalachh, data=traindata,family="binomial")
summary(model1)


#predict with a set seed
pred <- predict(model1,newdata=testdata,type="response")

#for loop for seeing the best threshold (best is a compromise between highest accuracy and least false negatives)
tb1<-rep(0,20)
ac1 <-rep(0,20)
tb5 <- rep(0,20)
for(i in 1:20){
  ypred.i <- ifelse(pred>(i*0.05),1,0)
  prednum <- factor(ypred.i,levels=c(0,1))
  ac1[i] <- mean(prednum==testdata$output)
  currtable <- table(prednum,testdata$output)
  tb1[i] <- currtable[2,1]
  tb5[i] <- currtable[1,2]
}

#normalizing the lists of values from the for loop to plot them on the same graph
nor <- function(x) {(x - min(x))/(max(x)-min(x))}
ac1[13] #showing the maximum accuracy with hindsight
tb1 <- nor(tb1) 
ac1 <- nor(ac1)
tb5 <- nor(tb5)


#plotting the graph to visualize the accuracy, number of false negatives and positives on the same plot (Figures 2b and 5)
plot(tb1,type="b",pch=19,xlab="Threshold value *0.05", col="red",ylab="Normalized values")
lines(ac1,pch=19,type="b",xlab="",ylab="",col="green")
lines(tb5,pch=19,type="b",xlab="",ylab="",col="blue")
legend(x="bottomleft",c("False Positives","Accuracy","False Negatives"), col=c("red","green","blue"),lty=1)




#Using the best model (compromise between high accuracy and low false negatives)
ypred.i <- ifelse(pred>(0.65),1,0)
prednum <- factor(ypred.i,levels=c(0,1))
table(prednum,testdata$output)

```


```{r}
#SVM
##Perform SVM model on train data and select kernel. Obtain prediction and confusion matrix.
svm.model1 <- svm(output ~ age + cp + trtbps + chol + thalachh, data = traindata, kernel = "linear")
pred.svm.model1 <- predict(svm.model1,newdata = testdata)
table(pred.svm.model1,testdata$output)

#Accuracy for Linear kernel
mean(pred.svm.model1 == testdata$output)






##REPEAT PROCEDURE FOR EACH KERNEL TYPE TO MAKE COMPARISON

#Polynomial kernel
svm.model2 <- svm(output ~ age + cp + trtbps + chol + thalachh, data = traindata,  kernel = "polynomial")
pred.svm.model2 <- predict(svm.model2,newdata = testdata)
table(pred.svm.model2,testdata$output)

#Accuracy for Polynomial kernel
mean(pred.svm.model2 == testdata$output)






#Sigmoid kernel
svm.model3 <- svm(output ~ age + cp + trtbps + chol + thalachh, data = traindata, kernel = "sigmoid")
pred.svm.model3 <- predict(svm.model3,newdata = testdata)
table(pred.svm.model3,testdata$output)

#Accuracy for Sigmoid kernel
mean(pred.svm.model3 == testdata$output)






#For the radial kernel, the additional parameters of gamma and cost are also included, and the best tuned model is used to obtain the confusion matrix and the accuracy 
svm.model_tune <- tune.svm(output ~ age + cp + trtbps + chol + thalachh, data = traindata,
                           kernel = "radial",cost = 10^(-1:2), gamma = c(0.1,0.5,1,2,2))
best.svm = svm.model_tune$best.model
pred.svm.model <- predict(best.svm,newdata = testdata)
table(pred.svm.model,testdata$output)






#Summary of the tuned svm model provides information about the optimal cost and gamma values that yield the greatest accuracy.
summary(svm.model_tune)

#Accuracy of radial kernel with cost = 10,gamma = 0.1
mean(pred.svm.model == testdata$output)

```

